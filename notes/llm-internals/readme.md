# LLM 原理与架构

深入理解大语言模型的核心原理、关键论文和架构设计。

---

## 📚 文档目录

### 核心架构

| 文档                                              | 描述                           |
| ------------------------------------------------- | ------------------------------ |
| [Transformer 架构](./transformer-architecture.md) | 注意力机制、编码器-解码器结构  |
| [位置编码](./positional-encoding.md)              | 绝对/相对位置编码、RoPE、ALiBi |
| [注意力变体](./attention-variants.md)             | MHA、MQA、GQA、Flash Attention |

### 训练方法

| 文档                         | 描述                               |
| ---------------------------- | ---------------------------------- |
| [预训练](./pre-training.md)  | 语言建模目标、数据集、Tokenization |
| [微调技术](./fine-tuning.md) | SFT、LoRA、QLoRA、Adapter          |
| [对齐技术](./alignment.md)   | RLHF、DPO、Constitutional AI       |

### 模型架构

| 文档                                | 描述                    |
| ----------------------------------- | ----------------------- |
| [GPT 系列](./gpt-series.md)         | GPT-1/2/3/4 架构演进    |
| [开源模型](./open-source-models.md) | LLaMA、Mistral、Qwen 等 |
| [MoE 架构](./mixture-of-experts.md) | 混合专家模型原理        |

### 关键论文

| 文档                                  | 描述               |
| ------------------------------------- | ------------------ |
| [必读论文列表](./essential-papers.md) | 领域里程碑论文汇总 |
| [论文精读笔记](./paper-notes.md)      | 重点论文详细解读   |

### 扩展话题

| 文档                                    | 描述                      |
| --------------------------------------- | ------------------------- |
| [Scaling Laws](./scaling-laws.md)       | 规模定律、Chinchilla 定律 |
| [推理优化](./inference-optimization.md) | KV Cache、量化、投机解码  |
| [长上下文](./long-context.md)           | 上下文扩展技术            |

---

## 🗺️ 学习路线

```
1. Transformer 基础
   └── Transformer 架构 → 位置编码 → 注意力变体

2. 训练流程
   └── 预训练 → 微调技术 → 对齐技术

3. 模型演进
   └── GPT 系列 → 开源模型 → MoE 架构

4. 进阶主题
   └── Scaling Laws → 推理优化 → 长上下文
```

---

## 📊 文档状态

| 文档             | 状态      | 完成度 |
| ---------------- | --------- | ------ |
| Transformer 架构 | 🚧 待编写 | 0%     |
| 位置编码         | 🚧 待编写 | 0%     |
| 注意力变体       | 🚧 待编写 | 0%     |
| 预训练           | 🚧 待编写 | 0%     |
| 微调技术         | 🚧 待编写 | 0%     |
| 对齐技术         | 🚧 待编写 | 0%     |
| GPT 系列         | 🚧 待编写 | 0%     |
| 开源模型         | 🚧 待编写 | 0%     |
| MoE 架构         | 🚧 待编写 | 0%     |
| 必读论文列表     | 🚧 待编写 | 0%     |
| 论文精读笔记     | 🚧 待编写 | 0%     |
| Scaling Laws     | 🚧 待编写 | 0%     |
| 推理优化         | 🚧 待编写 | 0%     |
| 长上下文         | 🚧 待编写 | 0%     |
