<!--
- [INPUT]: ä¾èµ– notes/llm-internals/CLAUDE.md çš„æ¨¡å—å®šä½ä¸ç´¢å¼•
- [OUTPUT]: è¾“å‡º æ¨ç†ä¼˜åŒ– æ–‡æ¡£
- [POS]: ä½äº notes/llm-internals æ¨¡å—çš„ æ¨ç†ä¼˜åŒ– ç¬”è®°
- [PROTOCOL]: å˜æ›´æ—¶æ›´æ–°æ­¤å¤´éƒ¨ï¼Œç„¶åæ£€æŸ¥ CLAUDE.md
-->

# æ¨ç†ä¼˜åŒ–

æé«˜ LLM æ¨ç†æ•ˆç‡çš„å…³é”®æŠ€æœ¯ã€‚

---

## ğŸ“ ç« èŠ‚å¤§çº²

### 1. KV Cache

- åŸç†ï¼šç¼“å­˜æ³¨æ„åŠ›è®¡ç®—
- å†…å­˜å ç”¨åˆ†æ
- ä¼˜åŒ–ç­–ç•¥

### 2. é‡åŒ–

- INT8/INT4 é‡åŒ–
- GPTQã€AWQã€GGUF
- ç²¾åº¦ä¸é€Ÿåº¦æƒè¡¡

### 3. æŠ•æœºè§£ç 

- å°æ¨¡å‹è‰ç¨¿ + å¤§æ¨¡å‹éªŒè¯
- åŠ é€ŸåŸç†
- å®ç°æ–¹æ³•

### 4. æ‰¹å¤„ç†ä¼˜åŒ–

- Continuous Batching
- PagedAttention (vLLM)
- Dynamic Batching

### 5. å…¶ä»–æŠ€æœ¯

- æ¨¡å‹è’¸é¦
- å‰ªæ
- ç®—å­èåˆ

### 6. å·¥å…·ç”Ÿæ€

- vLLM
- TensorRT-LLM
- llama.cpp

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [ ] Efficient Memory Management for LLM Serving (PagedAttention)
- [ ] GPTQ: Accurate Quantization for Generative Pre-trained Transformers
