# å¼€æºæ¨¡å‹

ä¸»æµå¼€æºå¤§è¯­è¨€æ¨¡å‹æ¶æ„åˆ†æã€‚

---

## ğŸ“ ç« èŠ‚å¤§çº²

### 1. LLaMA ç³»åˆ—

- LLaMA 1: RoPE, SwiGLU, RMSNorm
- LLaMA 2: GQA, æ‰©å±•ä¸Šä¸‹æ–‡
- LLaMA 3: æ›´å¤§è§„æ¨¡

### 2. Mistral

- Sliding Window Attention
- é«˜æ•ˆæ¶æ„è®¾è®¡
- Mixtral (MoE ç‰ˆæœ¬)

### 3. Qwen ç³»åˆ—

- Qwen 1/1.5/2/2.5/3
- å¤šæ¨¡æ€æ‰©å±•
- ä¸­æ–‡ä¼˜åŒ–

### 4. DeepSeek

- DeepSeek-V2/V3
- MLA (Multi-head Latent Attention)
- MoE æ¶æ„

### 5. å…¶ä»–æ¨¡å‹

- Gemma (Google)
- Phi (Microsoft)
- Yi (01.AI)

### 6. æ¶æ„å¯¹æ¯”

| æ¨¡å‹    | æ³¨æ„åŠ›    | ä½ç½®ç¼–ç  | æ¿€æ´»å‡½æ•° | å½’ä¸€åŒ–  |
| ------- | --------- | -------- | -------- | ------- |
| LLaMA   | GQA       | RoPE     | SwiGLU   | RMSNorm |
| Mistral | GQA + SWA | RoPE     | SwiGLU   | RMSNorm |
| Qwen    | GQA       | RoPE     | SwiGLU   | RMSNorm |

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [ ] LLaMA: Open and Efficient Foundation Language Models
- [ ] Mistral 7B
- [ ] Qwen Technical Report
