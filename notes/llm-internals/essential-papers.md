# å¿…è¯»è®ºæ–‡åˆ—è¡¨

LLM é¢†åŸŸé‡Œç¨‹ç¢‘è®ºæ–‡æ±‡æ€»ã€‚

---

## ğŸ›ï¸ åŸºç¡€æ¶æ„

| è®ºæ–‡                                                       | å¹´ä»½ | é‡è¦æ€§     | çŠ¶æ€ |
| ---------------------------------------------------------- | ---- | ---------- | ---- |
| Attention Is All You Need                                  | 2017 | â­â­â­â­â­ | [ ]  |
| BERT: Pre-training of Deep Bidirectional Transformers      | 2018 | â­â­â­â­â­ | [ ]  |
| GPT-2: Language Models are Unsupervised Multitask Learners | 2019 | â­â­â­â­â­ | [ ]  |
| GPT-3: Language Models are Few-Shot Learners               | 2020 | â­â­â­â­â­ | [ ]  |

---

## ğŸš€ è§„æ¨¡ä¸æ•ˆç‡

| è®ºæ–‡                                       | å¹´ä»½ | é‡è¦æ€§     | çŠ¶æ€ |
| ------------------------------------------ | ---- | ---------- | ---- |
| Scaling Laws for Neural Language Models    | 2020 | â­â­â­â­â­ | [ ]  |
| Training Compute-Optimal LLMs (Chinchilla) | 2022 | â­â­â­â­â­ | [ ]  |
| FlashAttention                             | 2022 | â­â­â­â­   | [ ]  |
| LoRA                                       | 2021 | â­â­â­â­â­ | [ ]  |

---

## ğŸ¯ å¯¹é½ä¸å®‰å…¨

| è®ºæ–‡                                 | å¹´ä»½ | é‡è¦æ€§     | çŠ¶æ€ |
| ------------------------------------ | ---- | ---------- | ---- |
| InstructGPT                          | 2022 | â­â­â­â­â­ | [ ]  |
| Constitutional AI                    | 2022 | â­â­â­â­   | [ ]  |
| Direct Preference Optimization (DPO) | 2023 | â­â­â­â­â­ | [ ]  |

---

## ğŸ¦™ å¼€æºæ¨¡å‹

| è®ºæ–‡               | å¹´ä»½ | é‡è¦æ€§     | çŠ¶æ€ |
| ------------------ | ---- | ---------- | ---- |
| LLaMA              | 2023 | â­â­â­â­â­ | [ ]  |
| Mistral 7B         | 2023 | â­â­â­â­   | [ ]  |
| Mixtral of Experts | 2024 | â­â­â­â­   | [ ]  |

---

## ğŸ“– é˜…è¯»é¡ºåºå»ºè®®

```
å…¥é—¨è·¯çº¿:
1. Attention Is All You Need
2. GPT-2 è®ºæ–‡
3. GPT-3 è®ºæ–‡
4. InstructGPT
5. LLaMA

è¿›é˜¶è·¯çº¿:
1. Scaling Laws
2. Chinchilla
3. FlashAttention
4. LoRA
5. DPO
```
