# GPT ç³»åˆ—

OpenAI GPT æ¨¡å‹çš„æ¶æ„æ¼”è¿›ã€‚

---

## ğŸ“ ç« èŠ‚å¤§çº²

### 1. GPT-1 (2018)

- æ¶æ„ï¼š12 å±‚ Transformer è§£ç å™¨
- å‚æ•°é‡ï¼š117M
- åˆ›æ–°ï¼šç”Ÿæˆå¼é¢„è®­ç»ƒ + å¾®è°ƒèŒƒå¼

### 2. GPT-2 (2019)

- æ¶æ„æ”¹è¿›
- å‚æ•°é‡ï¼š1.5B
- é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›

### 3. GPT-3 (2020)

- è§„æ¨¡æ‰©å±•ï¼š175B å‚æ•°
- ä¸Šä¸‹æ–‡å­¦ä¹  (In-Context Learning)
- Few-shot èƒ½åŠ›

### 4. GPT-4 (2023)

- å¤šæ¨¡æ€èƒ½åŠ›
- æ¶æ„çŒœæµ‹ (MoE?)
- æ€§èƒ½æå‡

### 5. æ¶æ„å¯¹æ¯”

| ç‰ˆæœ¬  | å‚æ•°é‡ | å±‚æ•° | éšè—ç»´åº¦ | ä¸Šä¸‹æ–‡é•¿åº¦ |
| ----- | ------ | ---- | -------- | ---------- |
| GPT-1 | 117M   | 12   | 768      | 512        |
| GPT-2 | 1.5B   | 48   | 1600     | 1024       |
| GPT-3 | 175B   | 96   | 12288    | 2048       |

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [ ] Improving Language Understanding by Generative Pre-Training (GPT-1)
- [ ] Language Models are Unsupervised Multitask Learners (GPT-2)
- [ ] Language Models are Few-Shot Learners (GPT-3)
