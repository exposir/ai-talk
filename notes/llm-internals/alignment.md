# 对齐技术

让模型符合人类意图和价值观。

---

## 📝 章节大纲

### 1. 对齐问题

- 什么是对齐
- 为什么需要对齐
- 对齐的目标

### 2. RLHF

- 奖励模型训练
- PPO 算法
- InstructGPT 流程

### 3. DPO

- 直接偏好优化
- 无需奖励模型
- 与 RLHF 对比

### 4. Constitutional AI

- 自我批评与修正
- AI 反馈

### 5. 其他方法

- RLAIF
- KTO
- IPO

### 6. 对齐评估

- 安全性评估
- 有用性评估
- 诚实性评估

---

## 📚 参考资料

- [ ] Training language models to follow instructions (InstructGPT)
- [ ] Direct Preference Optimization (DPO)
- [ ] Constitutional AI: Harmlessness from AI Feedback
