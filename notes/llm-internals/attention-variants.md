# æ³¨æ„åŠ›å˜ä½“

æé«˜æ•ˆç‡å’Œæ€§èƒ½çš„æ³¨æ„åŠ›æœºåˆ¶æ”¹è¿›ã€‚

---

## ğŸ“ ç« èŠ‚å¤§çº²

### 1. å¤šå¤´æ³¨æ„åŠ› (MHA)

- åŸç†ä¸å®ç°
- å‚æ•°é‡åˆ†æ

### 2. å¤šæŸ¥è¯¢æ³¨æ„åŠ› (MQA)

- K/V å…±äº«
- æ¨ç†åŠ é€Ÿæ•ˆæœ

### 3. åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA)

- MHA ä¸ MQA çš„æŠ˜ä¸­
- LLaMA 2 çš„åº”ç”¨

### 4. Flash Attention

- IO æ„ŸçŸ¥ç®—æ³•
- å†…å­˜æ•ˆç‡ä¼˜åŒ–
- Flash Attention 2/3

### 5. ç¨€ç–æ³¨æ„åŠ›

- Local Attention
- Sliding Window
- Longformer æ¨¡å¼

### 6. çº¿æ€§æ³¨æ„åŠ›

- é™ä½å¤æ‚åº¦åˆ° O(n)
- å…¸å‹æ–¹æ³•ä¸å±€é™

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [ ] Fast Transformer Decoding: One Write-Head is All You Need (MQA)
- [ ] GQA: Training Generalized Multi-Query Transformer
- [ ] FlashAttention: Fast and Memory-Efficient Exact Attention
