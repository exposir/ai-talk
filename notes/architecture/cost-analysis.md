<!--
- [INPUT]: 依赖 notes/architecture/CLAUDE.md 的模块定位与索引
- [OUTPUT]: 输出 2025 双引擎架构用量与成本模型分析 (Cost & Usage Model) 文档
- [POS]: 位于 notes/architecture 模块的 2025 双引擎架构用量与成本模型分析 (Cost & Usage Model) 笔记
- [PROTOCOL]: 变更时更新此头部，然后检查 CLAUDE.md
-->

# 2025 双引擎架构用量与成本模型分析 (Cost & Usage Model)

本文档基于“双引擎（Dual-Engine）”架构，对系统的 Token 用量与运行成本进行理论推算。核心结论：通过**语义路由**与**分层处理**，相比“大一统”模型架构，综合成本可降低约
**60%~80%**。

---

## 1. 计费模型假设 (Based on 2025 Market Rates)

为了方便计算，我们采用以下典型费率（单位：$/1M Tokens）：

| 组件/模型           | 类型                 | Input Price | Output Price | 备注                                     |
| :------------------ | :------------------- | :---------- | :----------- | :--------------------------------------- |
| **Ingress/Guard**   | DeBERTa-v3-small     | $0.00       | $0.00        | 自部署小模型，仅计算 GPU 算力成本 (极低) |
| **Embedding**       | OpenAI text-3-small  | $0.02       | N/A          | 用于 Cache 与 Router                     |
| **System 1 (Fast)** | GPT-4o-mini / Flash  | $0.15       | $0.60        | 处理 80% 流量                            |
| **System 2 (Slow)** | GPT-4o / DeepSeek-R1 | $2.50       | $10.00       | 处理 20% 流量                            |
| **CoT Overhead**    | Internal reasoning   | N/A         | $10.00       | System 2 的隐式思维链 Token              |

---

## 2. 流量分层与用量测算

假设每日处理 **1,000,000 (100万)** 次用户请求，平均 Query 长度 500
Tokens，平均 Answer 长度 500 Tokens。

### 层级 A: 语义缓存 (Semantic Cache) - L0

- **命中率预期**：20% (高频问答/重复Query)
- **单次消耗**：500 Tokens (Embedding) -> $0.00001
- **对比 LLM**：便宜 15~250 倍。
- **每日成本**：20万请求 \* $0.00001 = **$2**

### 层级 B: 快车道 (System 1) - 简单任务

- **路由占比**：60% (事实查询、闲聊、简单工具)
- **单次消耗**：
  - Input: 1000 (含 System Prompt)
  - Output: 300
- **单次成本**：(1k _ 0.15 + 0.3k _ 0.60) / 1000 = **$0.00033**
- **每日成本**：60万请求 \* $0.00033 = **$198**

### 层级 C: 慢车道 (System 2) - 深度推理

- **路由占比**：20% (代码生成、复杂分析、长文写作)
- **单次消耗**：
  - Input: 3000 (含 RAG 上下文)
  - CoT (隐式): 1000 (推理过程)
  - Output: 1000 (最终答案)
- **单次成本**：(3k _ 2.5 + 2k _ 10) / 1000 = **$0.0275**
  - _注：CoT Token 通常按 Output 计费_
- **每日成本**：20万请求 \* $0.0275 = **$5,500**

---

## 3. 综合成本对比 (每日 100万 请求)

| 架构方案                 | 处理策略                          | 每日总成本 (预估) | 平均单次请求成本 |
| :----------------------- | :-------------------------------- | :---------------- | :--------------- |
| **纯大模型 (Baseline)**  | 100% 全部走 GPT-4o                | ~$22,500          | $0.0225          |
| **双引擎 (Dual-Engine)** | 20% Cache + 60% Sym-1 + 20% Sym-2 | **$5,700**        | **$0.0057**      |

> **结论：仅需纯大模型方案 **25%** 的成本。**

---

## 4. 隐形 Token 消耗陷阱与优化

在实际“用量”中，以下部分容易被忽视：

### 1. RAG 膨胀

- **问题**：检索回来的切片往往包含大量无关信息，导致 System 2 Input 暴涨。
- **对策**：使用 Contextual Compression 或 Rerank 截断，强制 Top-K=3~5。

### 2. Guardrails 消耗

- **问题**：如果用 LLM 做 Input/Output 审核，成本会翻倍。
- **对策**：必须使用 **DeBERTa** 或 **BERT**
  类小模型做分类，不做生成，成本几乎为 0。

### 3. Tool Definition 冗余

- **问题**：每次请求都带上 50+ 个工具定义，占用 2k Context。
- **对策**：**Prompt Caching**。Anthropic 支持对 Tool
  Definition 缓存，首字成本降低 90%。
